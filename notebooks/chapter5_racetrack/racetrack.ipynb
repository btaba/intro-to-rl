{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib inline\n",
    "\n",
    "from tracks import fancytrack, fancytrack2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from tempfile import NamedTemporaryFile\n",
    "# from IPython.display import HTML\n",
    "\n",
    "# VIDEO_TAG = \"\"\"<video controls>\n",
    "#  <source src=\"data:video/x-m4v;base64,{0}\" type=\"video/mp4\">\n",
    "#  Your browser does not support the video tag.\n",
    "# </video>\"\"\"\n",
    "\n",
    "# def anim_to_html(anim):\n",
    "#     if not hasattr(anim, '_encoded_video'):\n",
    "#         with NamedTemporaryFile(suffix='.mp4') as f:\n",
    "#             anim.save(f.name, fps=10, extra_args=['-vcodec', 'libx264'])\n",
    "#             video = open(f.name, \"rb\").read()\n",
    "#         anim._encoded_video = video.encode(\"base64\")\n",
    "    \n",
    "#     return VIDEO_TAG.format(anim._encoded_video)\n",
    "\n",
    "# def display_animation(anim):\n",
    "#     plt.close(anim._fig)\n",
    "#     return HTML(anim_to_html(anim))\n",
    "\n",
    "# # automatically display animations\n",
    "# animation.Animation._repr_html_ = anim_to_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Track(object):\n",
    "    def __init__(self, track):\n",
    "        \"\"\"\n",
    "            0 = off track\n",
    "            1 = road - on track\n",
    "            2 = start line\n",
    "            3 = finish line\n",
    "        \"\"\"\n",
    "        self.track = track\n",
    "\n",
    "    def get_next_position(self, racecar):\n",
    "        \"\"\"\n",
    "            RaceCar racecar: RaceCar object\n",
    "        \"\"\"\n",
    "        \n",
    "        reward = -1\n",
    "        crash = False\n",
    "        \n",
    "        new_x = racecar.x + racecar.velocity_x\n",
    "        new_y = racecar.y + racecar.velocity_y\n",
    "        \n",
    "        final_x = new_x\n",
    "        final_y = new_y\n",
    "        \n",
    "        # Compute all the unique boxes we hit on a line between the start and end points\n",
    "        x_positions = np.linspace(racecar.x, new_x, num=20)\n",
    "        y_positions = np.linspace(racecar.y, new_y, num=20)\n",
    "        positions = zip(x_positions, y_positions)\n",
    "        positions = [(np.floor(x), np.floor(y)) for x, y in positions]\n",
    "        \n",
    "        # Get unique discrete positions visited during this time step\n",
    "        ordered_positions = []\n",
    "        for pos in positions:\n",
    "            if len(ordered_positions) == 0 or pos != ordered_positions[-1]:\n",
    "                ordered_positions.append(pos)\n",
    "                        \n",
    "        # Check if the car crashes into the track at any of those time points\n",
    "        #   or if it reached the finish line\n",
    "        for pos_idx, pos in enumerate(ordered_positions):\n",
    "\n",
    "            # ability to speed past the finish without penalty\n",
    "            if self.is_terminal_state_from_coordinates(pos[0], pos[1]):\n",
    "                reward = -1\n",
    "                final_x, final_y = ordered_positions[pos_idx]\n",
    "                break\n",
    "            \n",
    "            # check if the car crashes\n",
    "            if self.is_out_of_bounds(pos):\n",
    "                reward -= 5\n",
    "                crash_x, crash_y = pos\n",
    "                final_x, final_y = ordered_positions[pos_idx - 1]\n",
    "                racecar.velocity_x = 0\n",
    "                racecar.velocity_y = 0\n",
    "                crash = True\n",
    "                break\n",
    "            \n",
    "\n",
    "        # If the car is not moving, the car must move at least 1 step\n",
    "        # Here instead, we just give a negative reward for not moving, since we have left turns as well\n",
    "        if final_x == racecar.x and final_y == racecar.y:\n",
    "            reward -= 5\n",
    "#             if self.is_out_of_bounds((final_x + 1, final_y)):\n",
    "#                 final_y += 1\n",
    "#                 racecar.velocity_y = 1\n",
    "#             elif self.is_out_of_bounds((final_x, final_y + 1)):\n",
    "#                 final_x += 1\n",
    "#                 racecar.velocity_x = 1\n",
    "#             else:\n",
    "#                 random_choice = np.random.choice([0, 1])\n",
    "#                 final_x += random_choice\n",
    "#                 final_y += (1 - random_choice)\n",
    "#                 racecar.velocity_x += random_choice\n",
    "#                 racecar.velocity_y += (1 - random_choice)                    \n",
    "        \n",
    "        racecar.x = final_x\n",
    "        racecar.y = final_y\n",
    "        \n",
    "        return reward, crash\n",
    "\n",
    "    def convert_cartesian_to_indexes(self, x, y):\n",
    "        y_prime, x_prime = x, y\n",
    "        x_prime = self.track.shape[0] - x_prime - 1\n",
    "        return int(x_prime), int(y_prime)\n",
    "    \n",
    "    def convert_indexes_to_cartesian(self, x, y):\n",
    "        y_prime, x_prime = x, y\n",
    "        y_prime = self.track.shape[0] - y_prime - 1\n",
    "        return int(x_prime), int(y_prime)\n",
    "    \n",
    "    def is_terminal_state(self, racecar):\n",
    "        x, y = self.convert_cartesian_to_indexes(racecar.x, racecar.y)\n",
    "        if self.track[x, y] == 3:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_terminal_state_from_coordinates(self, x, y):\n",
    "        if self.is_out_of_bounds((x, y)):\n",
    "            return False\n",
    "\n",
    "        x, y = self.convert_cartesian_to_indexes(x, y)\n",
    "        if self.track[x, y] == 3:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_out_of_bounds(self, position):\n",
    "        x, y = position\n",
    "        \n",
    "        if x < 0 or x >= self.track.shape[1]:\n",
    "            return True\n",
    "        \n",
    "        if y < 0 or y >= self.track.shape[0]:\n",
    "            return True\n",
    "\n",
    "        # y is reversed in our frame of reference\n",
    "        x, y = self.convert_cartesian_to_indexes(x, y)\n",
    "\n",
    "        if self.track[x, y] == 0:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_random_start(self):\n",
    "        # returns x and y coordinates of random start\n",
    "        starts = np.argwhere(self.track == 2)\n",
    "        random_start = np.random.randint(len(starts))\n",
    "        start = starts[random_start]\n",
    "        return self.convert_indexes_to_cartesian(*start)\n",
    "    \n",
    "    def get_states(self):\n",
    "        return [self.convert_indexes_to_cartesian(x, y) for x, y in np.argwhere(self.track != 0)]\n",
    "    \n",
    "    def print_track(self, x, y):\n",
    "        x, y = self.convert_cartesian_to_indexes(x, y)\n",
    "        pt = np.copy(self.track)\n",
    "        pt[x, y] = -1\n",
    "        print(pt)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RaceCar(object):\n",
    "    def __init__(self):\n",
    "        self.velocity_x = 0\n",
    "        self.velocity_y = 0\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        \n",
    "        self.MAX_VELOCITY = 5\n",
    "        self.MIN_VELOCITY = -5\n",
    "\n",
    "    def get_episode(self, pi, track, actions, states, greedy=False, verbose=False, max_episodes=200):\n",
    "        \"\"\"\n",
    "            actions: an index to action dictionary\n",
    "            pi: numpy array of probabilities to take an action given the state\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.velocity_x = 0; self.velocity_y = 0\n",
    "        self.x, self.y = track.get_random_start()\n",
    "\n",
    "        saved_actions = []\n",
    "        crash_events = [False]\n",
    "        rewards = [0]\n",
    "        visited_states = [((self.x, self.y), (self.velocity_x, self.velocity_y))]\n",
    "        visited_positions = set([(self.x, self.y)])\n",
    "        \n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            state_idx = states[((self.x, self.y), (self.velocity_x, self.velocity_y))]\n",
    "\n",
    "            # choose greedy action with probability pi\n",
    "            if greedy:\n",
    "                action_idx = np.where(pi[state_idx, :] == np.amax(pi[state_idx, :]))[0]\n",
    "                action_idx = np.random.choice(action_idx)   \n",
    "            else:\n",
    "                action_idx = np.random.choice(len(actions), size=1, p=pi[state_idx, :])[0]    \n",
    "            \n",
    "            action = actions[action_idx]\n",
    "            saved_actions.append(action)\n",
    "            \n",
    "            # Take the action\n",
    "            self.velocity_x += action[0]\n",
    "            self.velocity_y += action[1]\n",
    "            self.velocity_x = min(max(self.velocity_x, self.MIN_VELOCITY), self.MAX_VELOCITY)\n",
    "            self.velocity_y = min(max(self.velocity_y, self.MIN_VELOCITY), self.MAX_VELOCITY)\n",
    "\n",
    "            # check if the car crashed, didn't move, or if we should end the episode, and penalize the rewards\n",
    "            reward, crash = track.get_next_position(self)\n",
    "            if (self.x, self.y) in visited_positions:\n",
    "                reward -= 6\n",
    "                # pass\n",
    "            if len(visited_states) > max_episodes:\n",
    "                terminated = True\n",
    "            else:\n",
    "                terminated = track.is_terminal_state(self)\n",
    "            \n",
    "            # save the rewards, states, and actions\n",
    "            crash_events.append(crash)\n",
    "            rewards.append(reward)\n",
    "            visited_states.append(((self.x, self.y), (self.velocity_x, self.velocity_y)))\n",
    "            visited_positions.update([(self.x, self.y)])\n",
    "            if terminated: saved_actions.append((0,0))\n",
    "            \n",
    "            if verbose:\n",
    "                track.print_track(self.x, self.y)\n",
    "                print('Velocity is now: ', (self.velocity_x, self.velocity_y))\n",
    "        \n",
    "        return visited_states, saved_actions, rewards, crash_events\n",
    "        \n",
    "    def get_states(self):\n",
    "        return list(product(\n",
    "                range(self.MIN_VELOCITY, self.MAX_VELOCITY + 1),\n",
    "                range(self.MIN_VELOCITY, self.MAX_VELOCITY + 1)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MonteCarlo(object):\n",
    "    def __init__(self, actions, agent, environment):\n",
    "        self.actions_list = actions\n",
    "        self.agent = agent\n",
    "        self.environment = environment\n",
    "        \n",
    "        self.actions_to_idx = {action: idx for idx, action in enumerate(self.actions_list)}\n",
    "        self.idx_to_actions = {idx: action for idx, action in enumerate(self.actions_list)}\n",
    "\n",
    "        self.states_list = list(product(environment.get_states(), agent.get_states()))\n",
    "        self.states_to_idx = {state: idx for idx, state in enumerate(self.states_list)}\n",
    "\n",
    "        self.initialize_random_policy()\n",
    "        \n",
    "    def initialize_random_policy(self):\n",
    "        self.Q = np.random.random((len(self.states_to_idx), len(self.actions_to_idx)))\n",
    "        self.Returns = {(s, a): [] for s, a in product(self.states_to_idx, self.actions_to_idx)}\n",
    "\n",
    "        self.pi = np.random.random((len(self.states_to_idx), len(self.actions_to_idx)))\n",
    "        self.pi = self.pi / np.sum(self.pi, axis=1)[:, None]\n",
    "    \n",
    "    def apply_discount(self, r, gamma):\n",
    "        for i, rr in enumerate(r):\n",
    "            r[i] = (gamma ** i) * rr\n",
    "        return r\n",
    "    \n",
    "    def on_policy_learning(self, num_iterations, epsilon=.1, gamma=1, verbose=False, \n",
    "                           sample_every=1000, get_greedy_episode_after=500):\n",
    "        \"\"\"\n",
    "            epsilon: sets minimum probability threshold for policy pi\n",
    "            gamma: discount factor in rewards\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        count = 0\n",
    "        learning = True\n",
    "        \n",
    "        movie_array = []\n",
    "\n",
    "        while learning:\n",
    "            if count % sample_every == 0 and verbose:\n",
    "                if count > get_greedy_episode_after:\n",
    "                    get_greedy = True\n",
    "                else:\n",
    "                    get_greedy = False\n",
    "\n",
    "                visited_states, actions_taken, rewards, crash_events = self.agent.get_episode(\n",
    "                    self.pi, \n",
    "                    self.environment, \n",
    "                    self.idx_to_actions, \n",
    "                    self.states_to_idx,\n",
    "                    greedy=get_greedy\n",
    "                )\n",
    "                movie_array.append((visited_states, actions_taken, rewards, crash_events, count))\n",
    "                count += 1\n",
    "                continue\n",
    "            else:\n",
    "                visited_states, actions_taken, rewards, crash_events = self.agent.get_episode(\n",
    "                    self.pi, \n",
    "                    self.environment, \n",
    "                    self.idx_to_actions, \n",
    "                    self.states_to_idx\n",
    "                )                \n",
    "\n",
    "            has_visited_first_occurence = {}\n",
    "            for idx, sa in enumerate(zip(visited_states, actions_taken)):\n",
    "                s, a = sa\n",
    "                if (s, a) not in has_visited_first_occurence:\n",
    "                    r = self.apply_discount(rewards[idx:], gamma)\n",
    "                    self.Returns[(s, a)].append(sum(r))\n",
    "                    self.Q[self.states_to_idx[s], self.actions_to_idx[a]] = np.mean(self.Returns[(s, a)]) \n",
    "                    has_visited_first_occurence[(s, a)] = 0\n",
    "\n",
    "            for s in visited_states:\n",
    "                # We can take the greedy action, but it's probably better to break ties\n",
    "                # a_star = np.argmax(Q[states_to_idx[s],:])\n",
    "                action_idx = np.where(self.Q[self.states_to_idx[s],:] == np.amax(self.Q[self.states_to_idx[s],:]))[0]\n",
    "                a_star = np.random.choice(action_idx)\n",
    "                for action_idx, a in enumerate(self.actions_list):\n",
    "                    if a_star == action_idx:\n",
    "                        self.pi[self.states_to_idx[s], action_idx] = 1 - epsilon + epsilon / len(self.actions_list)\n",
    "                    else:\n",
    "                        self.pi[self.states_to_idx[s], action_idx] = epsilon / len(self.actions_list)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            if count >= num_iterations: learning = False\n",
    "        \n",
    "        if verbose:\n",
    "            return movie_array\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the environment and agent\n",
    "car = RaceCar()\n",
    "track = Track(fancytrack)\n",
    "\n",
    "actions_list = list(product([-1, 0, 1], [-1, 0, 1]))\n",
    "mc = MonteCarlo(actions_list, car, track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learn by On-Policy Monte Carlo\n",
    "movie_array = mc.on_policy_learning(num_iterations=5000, verbose=True, gamma=.8, epsilon=0.20, \n",
    "                                    sample_every=499, get_greedy_episode_after=3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAD7CAYAAADzaviDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12086a210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a movie!!\n",
    "\n",
    "from matplotlib.patches import FancyArrow\n",
    "# Get the number of frames and initialize the track image\n",
    "\n",
    "track_matrix = np.copy(track.track)\n",
    "track_matrix[-1, -1] = 4\n",
    "\n",
    "# Create the figure of the track\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, autoscale_on=False, xlim=(0, track_matrix.shape[1] - 1), ylim=(0 ,track_matrix.shape[0] - 1))\n",
    "ax.grid()\n",
    "im = ax.imshow(np.flipud(track_matrix), origin='upper', interpolation='none')\n",
    "\n",
    "# Make an info box\n",
    "bbox_props = dict(boxstyle=\"round4,pad=0.3\", fc=\"white\", ec=\"b\", lw=2)\n",
    "info_template = 'Episode: %d \\n Time: %d \\n Reward: %d \\n X_Velocity: %d \\n Y_Velocity: %d'\n",
    "\n",
    "# annotation\n",
    "annotation = ax.annotate(\n",
    "    info_template %(0, 0, 0, 0, 0),\n",
    "    xy=(track_matrix.shape[0] - 2.5, .5),\n",
    "    bbox=bbox_props\n",
    ")\n",
    "annotation.set_animated(True)\n",
    "\n",
    "\n",
    "def frame_generator():\n",
    "    for m_idx, m in enumerate(movie_array):\n",
    "        visited_states, _, _, _, _ = m\n",
    "        for vs_idx, vs in enumerate(visited_states):\n",
    "            yield (m_idx, vs_idx)\n",
    "\n",
    "\n",
    "def updatetrack(idx):\n",
    "    m_idx, vs_idx = idx\n",
    "    m = movie_array[m_idx]\n",
    "    track_matrix = np.copy(track.track)\n",
    "    visited_states, actions_taken, rewards_given, had_crash, episode_count = m\n",
    "    state = visited_states[vs_idx]\n",
    "    x, y = track.convert_cartesian_to_indexes(state[0][0], state[0][1])\n",
    "    vx, vy = state[1][0], state[1][1]\n",
    "    \n",
    "    # place the car\n",
    "    if had_crash[vs_idx]:\n",
    "        track_matrix[x, y] = 3\n",
    "    else:\n",
    "        track_matrix[x, y] = 4\n",
    "\n",
    "    # get rewards\n",
    "    r = sum(rewards_given[:(vs_idx + 1)])\n",
    "    action = actions_taken[vs_idx]\n",
    "        \n",
    "    # update the figure\n",
    "    im.set_array(np.flipud(track_matrix))\n",
    "    annotation.set_text(info_template %(episode_count, vs_idx, r, vx, vy))\n",
    "    ax.patches = []\n",
    "    ax.add_patch(FancyArrow(\n",
    "        x=state[0][0], y=state[0][1],\n",
    "        dx=action[0], dy=action[1], \n",
    "        head_length=.2, head_width=.25, width=.1, edgecolor='white'\n",
    "    ))\n",
    "    \n",
    "    return im, annotation\n",
    "\n",
    "anim = animation.FuncAnimation(fig, updatetrack, frames=frame_generator, blit=False, save_count=9000)\n",
    "# display_animation(anim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.animation import FFMpegWriter\n",
    "anim.save('racecar.mp4', fps=60, writer=FFMpegWriter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
